{"cells":[{"cell_type":"markdown","source":["# Finetuning BERTimbau for regression\n","\n","Código baseado no tutorial do The Medium de finetuning de BERT para regressão encontrado \n","<a href=\"https://medium.com/ilb-labs-publications/fine-tuning-bert-for-a-regression-task-is-a-description-enough-to-predict-a-propertys-list-price-cf97cd7cb98a\">aqui<a/> "],"metadata":{"id":"fPrmvu2Fk_8a"}},{"cell_type":"markdown","source":["## Installing dependencies"],"metadata":{"id":"pxci22VOmJIv"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"NkPH8ZokmP2w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setting up variables"],"metadata":{"id":"wgAf73Yi_jmi"}},{"cell_type":"code","source":["DATA_PATH = r'/home/allan_m_ufms_br/tweets.csv'\n","PRETRAINED_MODEL = 'neuralmind/bert-base-portuguese-cased'"],"metadata":{"id":"U9aQqkcv_nl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFySzEM1kzmY"},"outputs":[],"source":["BATCH_SIZE = 8\n","NUM_EPOCHS = 10"]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"MMd1DYAMmhgK"}},{"cell_type":"markdown","source":["### Importing data"],"metadata":{"id":"2hjVR2FQmj14"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLTYisJakzmP"},"outputs":[],"source":["import pandas as pd\n","\n","cols = [\"Datetime\",\"Text\",\"Likes\",\"Retweets\"]\n","data = pd.read_csv(\n","    DATA_PATH,\n","    header=0,\n","    names=cols,\n","    engine=\"python\",\n","    encoding=\"utf-8\",\n","    index_col = False\n",")"]},{"cell_type":"markdown","source":["### Creating \"Engagement\" metric\n","\n","The engagement metric was created to simplify the problem by having a single value that represents both likes and retweets.\n","\n","By getting the average number of likes and dividing by the average number of retweets we got that on average a post gets eight times more likes than retweets.\n","\n","Since reach is way more important that likeness for a business, we decided that engagement would be the number of likes summed to eight times the number of retweets."],"metadata":{"id":"dRDdh6l9nTpV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHWRwtsukzmR"},"outputs":[],"source":["data = data.dropna()\n","\n","\n","data[\"Engagement\"] = data.Likes.astype(int) + (8 * data.Retweets.astype(int))\n","\n","\n","data.drop([\"Datetime\",\"Likes\",\"Retweets\"],\n","          axis=1,\n","          inplace=True)"]},{"cell_type":"markdown","source":["### Cleaning up data and tokenizing"],"metadata":{"id":"GmIa8cBLyF67"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlrzT-k0kzmS"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, do_lower_case=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64bb31O8kzmT"},"outputs":[],"source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import RSLPStemmer\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('rslp')\n","\n","def clean_tweet(tweet):\n","    # remove links\n","    tweet = re.sub(r'http(\\S)+', '', tweet)\n","    # remove pontuação\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","    # converte para minúsculas\n","    tweet = tweet.lower()\n","    emoji_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        \"]+\", flags=re.UNICODE)\n","    tweet = emoji_pattern.sub(r'', tweet)\n","    # remove stop words em português\n","    stop_words = set(stopwords.words('portuguese'))\n","    words = nltk.word_tokenize(tweet)\n","    words = [word for word in words if not word in stop_words]\n","    # aplica stemização\n","    stemmer = RSLPStemmer()\n","    words = [stemmer.stem(word) for word in words]\n","    # junta as palavras novamente\n","    tweet = ' '.join(words)\n","    tweet = tokenizer.tokenize(tweet)\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sn6zCfbKkzmU"},"outputs":[],"source":["data_clean = data.copy()\n","data_clean.Text = [clean_tweet(str(tweet)) for tweet in data.Text]"]},{"cell_type":"markdown","source":["### Normalizing the engagement metric"],"metadata":{"id":"wOo-d5wLyNZD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4h6IhNVokzmV"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","engagement_scaler = StandardScaler()\n","data_labels = np.array(data_clean['Engagement'].tolist())\n","data_labels = data_labels.reshape(-1, 1)\n","engagement_scaler.fit(data_labels)\n","\n","norm_labels = engagement_scaler.transform(data_labels)"]},{"cell_type":"markdown","source":["### Splitting data into train, test and validation"],"metadata":{"id":"ChIBqNkjyU4g"}},{"cell_type":"markdown","source":["First the data is shuffled"],"metadata":{"id":"WJ5E6W-H_OfJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GjuG_G-kzmW"},"outputs":[],"source":["TWEETS_USED_FOR_TRAINING = 100_000\n","\n","shuffle=np.random.randint(0,len(data_clean['Text']),TWEETS_USED_FOR_TRAINING)\n","\n","y_shuffled = np.array(norm_labels)[shuffle]\n","x_shuffled = np.array(data_clean['Text'])[shuffle]"]},{"cell_type":"markdown","source":["The it's separed into three parts"],"metadata":{"id":"NHoBOj0Y_SdA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"214xvmTkkzmX"},"outputs":[],"source":["import sklearn.model_selection as model_selection\n","\n","xtrain, xtest, ytrain, ytest = model_selection.train_test_split(x_shuffled, y_shuffled, test_size=0.30, random_state=42,shuffle=True)\n","xtrain, xval, ytrain, yval = model_selection.train_test_split(xtrain, ytrain, test_size=0.30, random_state=42,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKenF6kfkzmY"},"outputs":[],"source":["train_encodings = tokenizer(xtrain.tolist(), truncation=True, padding=True, max_length=512, is_split_into_words=True, return_tensors='pt')\n","test_encodings = tokenizer(xtest.tolist(), truncation=True, padding=True,max_length=512, is_split_into_words=True, return_tensors='pt')\n","val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True,max_length=512, is_split_into_words=True, return_tensors='pt')"]},{"cell_type":"markdown","source":["And lastly the dataloaders are created"],"metadata":{"id":"EYXDX-oW_Xnj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAvGeTVgkzmY"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","def create_dataloaders(inputs, masks, labels, batch_size):\n","    input_tensor = inputs.clone().detach()\n","    mask_tensor = masks.clone().detach()\n","    labels_tensor = torch.tensor(labels)\n","    dataset = TensorDataset(input_tensor, mask_tensor, labels_tensor)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    return dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpcNmCN-kzmZ"},"outputs":[],"source":["dl_train = create_dataloaders(train_encodings.input_ids, train_encodings.attention_mask, ytrain, BATCH_SIZE)\n","dl_test = create_dataloaders(test_encodings.input_ids, test_encodings.attention_mask, ytest, BATCH_SIZE)\n","dl_val = create_dataloaders(val_encodings.input_ids, val_encodings.attention_mask, yval, BATCH_SIZE)"]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"PP8pCkGCy3TM"}},{"cell_type":"markdown","source":["### Create model"],"metadata":{"id":"Fht9BLM8y-2k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCNvIbEYkzmZ"},"outputs":[],"source":["import torch.nn as nn\n","from typing import Tuple\n","\n","class BertEngagementRegressor(nn.Module):\n","    def __init__(self,model):\n","        super().__init__()\n","        self.bert = model.bert\n","        self.config = model.config\n","        self.linear = nn.Linear(self.config.hidden_size,200)\n","        self.dropout = nn.Dropout(p=0.1)\n","        self.linear2 = nn.Linear(200,1)\n","        self.double()\n","\n","    def forward(self, input_ids, attention_masks) ->Tuple[torch.Tensor]:\n","        output = self.bert(input_ids, attention_masks)[1]\n","        output = self.linear(output)\n","        output = self.dropout(output)\n","        output = self.linear2(output)\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtsBPYzhkzmZ"},"outputs":[],"source":["from transformers import AutoModelForPreTraining\n","\n","model_base= AutoModelForPreTraining.from_pretrained(PRETRAINED_MODEL)\n","model = BertEngagementRegressor(model=model_base)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8giOaEykzmZ"},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)"]},{"cell_type":"markdown","source":["### Setting up for training"],"metadata":{"id":"N4p2hUp6zbiv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9R2ODEf1kzma"},"outputs":[],"source":["from torch.optim import AdamW\n","\n","optimizer = AdamW(model.parameters(),lr=5e-5,eps=1e-8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QhGzMW2kzma"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","total_steps = len(dl_train) * NUM_EPOCHS\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bnyWZT5wkzma"},"outputs":[],"source":["loss_function = nn.MSELoss()"]},{"cell_type":"markdown","source":["### Training the model"],"metadata":{"id":"na41IQ6xzj6-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2_qx9mTkzmc"},"outputs":[],"source":["def evaluate(model, loss_function, test_dataloader, device):\n","    model.eval()\n","    test_loss, test_r2 = [], []\n","    for batch in test_dataloader:\n","        batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n","        batch_labels = torch.squeeze(batch_labels)\n","        with torch.no_grad():\n","            outputs = model(batch_inputs, batch_masks)\n","        loss = loss_function(outputs, batch_labels)\n","        test_loss.append(loss.item())\n","        r2 = r2_score(outputs, batch_labels)\n","        test_r2.append(r2.item())\n","    return test_loss, test_r2\n","\n","def r2_score(outputs, labels):\n","    labels_mean = torch.mean(labels)\n","    ss_tot = torch.sum((labels - labels_mean) ** 2)\n","    ss_res = torch.sum((labels - outputs) ** 2)\n","    r2 = 1 - ss_res / ss_tot\n","    return r2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBsviTZVkzmb"},"outputs":[],"source":["from torch.nn.utils.clip_grad import clip_grad_norm_\n","\n","def train(model, optimizer, scheduler, loss_function, train_dataloader, validation_dataloader, device,  clip_value=2):\n","    max_epoch_no_improve = 5\n","    cur_epoch = 0\n","    lowest_loss = 10000\n","    epochs_since_best = 0\n","    done = False\n","    loss_train, loss_val, val_r2 = [], []\n","    while not done:\n","      loss_epoch = []\n","      model.train()\n","      for step, batch in enumerate(train_dataloader):\n","          print(\"epoch:\", cur_epoch,\" - \", step,\"/\",len(train_dataloader))\n","          batch_inputs, batch_masks, batch_labels = tuple(b.to(device) for b in batch)\n","          model.zero_grad()\n","          outputs = model(batch_inputs, batch_masks)           \n","          loss = loss_function(outputs.squeeze(), batch_labels.squeeze())\n","          loss.backward()\n","          loss_epoch.append(loss.cpu().item())\n","          clip_grad_norm_(model.parameters(), clip_value)\n","          optimizer.step()\n","          scheduler.step()\n","      loss_train.append(np.mean(loss_epoch))\n","      loss, r2 = evaluate_loss(model, loss_function, validation_dataloader, device)\n","      mean_loss = np.mean(loss)\n","      val_r2.append(np.mean(r2))\n","      loss_val.append(mean_loss)\n","      epochs_since_best += 1\n","      if mean_loss < lowest_loss:\n","          lowest_loss = mean_loss\n","          epochs_since_best = 0\n","          print(\"Best model found! saving...\")\n","          torch.save(model.state_dict(),f'./regressor_state_dict_{cur_epoch}.pth')\n","          torch.save(model, f'./regressor_model_{cur_epoch}.pth')\n","      if epochs_since_best > max_epoch_no_improve:\n","          done = True\n","      print(\"epoch: %d, loss_train: %4.3f, last_best: %d\"%(cur_epoch,mean_loss,epochs_since_best))\n","      print(\"-----\")\n","      loss_train.append(mean_loss)\n","      cur_epoch += 1\n","    return model, loss_train, loss_val, r2_val"]},{"cell_type":"code","source":["model, loss_train, loss_val, r2_val = train(model, optimizer, scheduler, loss_function, dl_train, dl_val, device, clip_value=2)"],"metadata":{"id":"n8jvDFKR8sn0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qep5Sry-kzmb"},"outputs":[],"source":["torch.save(model, \"final_regressor_model.pth\")\n","with open(\"loss_values.txt\", \"a\") as file:\n","  file.write(\"loss_train:\",str(loss_train),\"\\n\")\n","  file.write(\"loss_val:\",str(loss_val),\"\\n\")\n","  file.write(\"r2_val:\",str(r2_val),\"\\n\")"]},{"cell_type":"markdown","source":["## Post-processing"],"metadata":{"id":"Jg8C9ueDzukS"}},{"cell_type":"markdown","source":["### Plotting losses"],"metadata":{"id":"4Wq3SehQ8zTz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMpRFlLy4kqN"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","x=range(len(loss_train))\n","ax.plot(x, loss_train, label='test_loss')\n","ax.plot(x, loss_val, label='test_r2')\n","\n","ax.set_xlabel('Epochs')\n","ax.set_ylabel('loss')\n","ax.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","source":["### Testing results with test dataset"],"metadata":{"id":"5lOLWEm6908c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fuYWBsw0kzmd"},"outputs":[],"source":["def predict(model, dataloader, device):\n","    model.eval()\n","    output = []\n","    for batch in dataloader:\n","        batch_inputs, batch_masks, _ = tuple(b.to(device) for b in batch)\n","        with torch.no_grad():\n","            output += model(batch_inputs, batch_masks).view(1,-1).tolist()[0]\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXYLCslikzmd"},"outputs":[],"source":["y_pred = predict(model, dl_test, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC81X9WBkzmd"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import median_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.metrics import r2_score\n","\n","mae = mean_absolute_error(ytest, y_pred)\n","print(f\"mean absolute error:{mae}\")\n","mdae = median_absolute_error(ytest, y_pred)\n","print(f\"median_absolute_error:{mdae}\")\n","mse = mean_squared_error(ytest, y_pred)\n","print(f\"mean_squared_error:{mse}\")\n","mape = mean_absolute_percentage_error(ytest, y_pred)\n","print(f\"mean_absolute_percentage_error:{mape}\")\n","mdape = ((pd.Series(ytest) - pd.Series(y_pred))\\pd.Series(ytest)).abs().median()\n","print(f\"median_absolute_percentage_error:{mdape}\")\n","r2 = r2_score(ytest, y_pred)\n","print(f\"regression_score:{r2}\")"]}],"metadata":{"kernelspec":{"display_name":"regressor","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}